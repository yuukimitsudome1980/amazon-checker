import requests
from bs4 import BeautifulSoup
import streamlit as st
import pandas as pd
import numpy as np
import sqlite3
import datetime
import time
import glob
import streamlit.components.v1 as components
from datetime import timedelta
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import plotly.graph_objs as go
import plotly.express as px
import plotly.io as pio
import seaborn as sns
import openpyxl
from io import BytesIO

st.set_page_config(layout="wide")

st.title("amazonèª¿ã¹ã‚‹å›ğŸ¤–")

name = st.sidebar.text_input("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", value="ã‚³ãƒ¼ãƒ’ãƒ¼è±†")
name = name.replace(' ','+')
name = name.replace('ã€€','+')

snum = st.sidebar.number_input('é–‹å§‹ãƒšãƒ¼ã‚¸',step=1, value=1)
num = st.sidebar.number_input('çµ‚äº†ãƒšãƒ¼ã‚¸', value=2)
num = num+1
waittime = st.sidebar.slider('èª­è¾¼å¾…æ©Ÿæ™‚é–“(ç§’)', 1, 10, 6)

df_list=[]
for page in range(snum,num):

    url = f"https://www.amazon.co.jp/s?k={name}&page={page}"

    # time.sleep(2)
    response = requests.get(url, timeout=3.5)
    html_content = response.content
    time.sleep(waittime)
    soup = BeautifulSoup(html_content, 'html.parser')
    rows = soup.find_all('div',class_='a-section a-spacing-small puis-padding-left-small puis-padding-right-small')
    page_list = []
    for row in rows:
        try:
            item_name = row.find('a').text
            # ä¾¡æ ¼
            element = row.find('span',class_="a-offscreen")
            if element:
                price = row.find('span',class_="a-offscreen").text
            else:
                price = "ãªã—"
            #å˜ä¾¡
            element = row.find_all('span',class_="a-size-base a-color-secondary")
            if element:
                unit_price = row.find_all('span',class_="a-size-base a-color-secondary")[-1].text
            else:
                unit_price = "ãªã—"
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼
            element = row.find('span',class_="a-icon-alt")
            if element:
                review = row.find('span',class_="a-icon-alt").text
            elif element is None:
                review = 0
            else:
                review = 0
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
            element = row.find('span',class_="a-size-base s-underline-text")
            if element:
                review_c = row.find('span',class_="a-size-base s-underline-text").text
            elif element is None:
                review_c = 0
            else:
                review_c = 0
            # é–²è¦§æ•°
            element = row.find('span',class_="a-size-base a-color-secondary")
            if element:
                view = row.find('span',class_="a-size-base a-color-secondary").text
            elif element is None:
                view = 0
            else:
                view = 0
            #é€æ–™
            element = row.find('span',class_="a-color-base a-text-bold")
            if element is None:
                element = "ãªã—"
            else:
                delivery = row.find('span',class_="a-color-base a-text-bold").text
            postage = row.find_all('span',class_="a-color-base")[-1].text
            link_list = []
            for href in row.find_all('a'):
                href = href.get('href')
                link_list.append(href)
            item_code = link_list[0]
            start = item_code.find("dp/") + 3
            end = item_code.find("?", start)
            item_code_result = item_code[start:]
            if len(item_code_result) > 10:
                end = item_code.find("?", start)
                item_code_result = item_code[start:end]
            review_url = f"https://www.amazon.co.jp/product-reviews/{item_code_result}/"
            
            d  = {
                'å•†å“å':item_name,
                'ä¾¡æ ¼':price,
                'ï¼‘å€‹å˜ä¾¡':unit_price,
                'ãƒ¬ãƒ“ãƒ¥ãƒ¼':review,
                'ãƒ¬ãƒ“ãƒ¥æ•°':review_c,
                'é–²è¦§æ•°':view,
                'ç´æœŸ':delivery,
                'é€æ–™':postage,
                'å‹ç•ª':item_code_result,
                'ãƒ¬ãƒ“ãƒ¼URL':review_url,
                'ãƒšãƒ¼ã‚¸':page
            }
            page_list.append(d)
            df = pd.DataFrame(page_list)
        except Exception as e:
            st.write(e)

# èª­ã¿è¾¼ã¿ã‚’ç¢ºèªã™ã‚‹ç‚ºURL,ãƒ‡ãƒ¼ã‚¿ï¼‘è¡Œç›®ã‚’è¡¨ç¤º
    st.write(url)
    st.write(df.head(1))
    df_list.append(df)

# ãƒªã‚¹ãƒˆã«å…¥ã£ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’çµåˆ
df_page = pd.concat(df_list)


df_review = pd.DataFrame(rev_list)
try:
    df_page['ä¾¡æ ¼'] = df_page['ä¾¡æ ¼'].str.replace('ï¿¥','')
    df_page['ï¼‘å€‹å˜ä¾¡'] = df_page['ï¼‘å€‹å˜ä¾¡'].str.replace('(','').str.replace(')','').str.replace('Amazonã¯æ—¥æœ¬ã®ä¸­å°ä¼æ¥­ã®ãƒ–ãƒ©ãƒ³ãƒ‰ã®å•†å“ã‚’å¿œæ´ã—ã¦ã„ã¾ã™ã€‚é£Ÿå“ã‹ã‚‰å®¶é›»ã¾ã§ã€Œæ—¥æœ¬ã®ä¸­å°ä¼æ¥­ å¿œæ´ã‚¹ãƒˆã‚¢ã€ã‚’ä»Šã™ããƒã‚§ãƒƒã‚¯ã€‚','').str.replace('ã“ã¡ã‚‰ã‹ã‚‰ã‚‚ã”è³¼å…¥ã„ãŸã ã‘ã¾ã™','')
    df_page['ãƒ¬ãƒ“ãƒ¥ãƒ¼'] = df_page['ãƒ¬ãƒ“ãƒ¥ãƒ¼'].str.replace('5ã¤æ˜Ÿã®ã†ã¡','')
    df_page['é–²è¦§æ•°'] = df_page['é–²è¦§æ•°'].str.replace('(','').str.replace(')','')
except Exception as e:
    st.write('æ¤œç´¢ä¸€è¦§ã®æ–‡å­—ç½®æ›ãŒã§ãã¾ã›ã‚“ã§ã—ãŸ  ')

        
df_page = df_page.reset_index(drop=True)

time.sleep(1)
count = len(df_page)-2
st.sidebar.subheader(f'è¡Œæ•°:{count}')
if name != "":
    st.dataframe(df_page)

# ã‚¨ã‚¯ã‚»ãƒ«ã§å‡ºåŠ›ã™ã‚‹ç‚ºé–¢æ•°ä½œæˆ
def df_to_xlsx(df):
    byte_xlsx = BytesIO()
    writer_xlsx = pd.ExcelWriter(byte_xlsx, engine="xlsxwriter")
    df.to_excel(writer_xlsx, index=False, sheet_name="Sheet1")
    ##-----å¿…è¦ã«å¿œã˜ã¦excelã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç­‰ã‚’è¨­å®š-----##
    workbook = writer_xlsx.book
    worksheet = writer_xlsx.sheets["Sheet1"]
    format1 = workbook.add_format({"num_format": "0.00"})
    worksheet.set_column("A:A", None, format1)
    writer_xlsx.save()
    ##---------------------------------------------##
    workbook = writer_xlsx.book
    out_xlsx = byte_xlsx.getvalue()
    return out_xlsx

df_xlsx = df_to_xlsx(df_page)
st.download_button(
    label="ã‚¨ã‚¯ã‚»ãƒ«å½¢å¼ ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    data = df_xlsx,
    file_name= f'{name}.xlsx',
)
